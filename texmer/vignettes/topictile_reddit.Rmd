---
title: "Segmenting Reddit Comments using TopicTile"
author: "Rasmus Holm"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

# Loading the required libraries
```{r}
## devtools::install_github("raholm/reddy", subdir="reddy")
## devtools::install_github("raholm/texcur", subdir="texcur")
## devtools::install_github("raholm/texmer", subdir="texmer")

suppressMessages(library(reddy))
suppressMessages(library(texcur))
suppressMessages(library(texmer))
suppressMessages(library(dplyr))
suppressMessages(library(mallet))
suppressMessages(library(Rcpp))
```

# Reading corpus

```{r}
filename <- paste(system.file("extdata", package="reddy"),
                  "RC_2008_100000_stream.json", sep="/")

corpus <- read_reddit_stream(filename) %>%
    select(c(id, body, link_id, subreddit, created_utc)) %>%
    rename(text=body, thread=link_id, date=created_utc)
```

# Reading stopwords

```{r}
stopwords_filename <- paste(system.file("extdata", package="reddy"),
                            "stopwords.txt", sep="/")

stopwords <- read.table(stopwords_filename, stringsAsFactors=FALSE)$V1
```

# Creating the corpus

```{r}
discussion_threads <- corpus %>%
    mutate(id=thread) %>%
    tf_tokenize() %>%
    arrange(date) %>%
    tf_merge_tokens(delim=" ") %>%
    filter(text != "deleted")

curated_discussion_threads <- discussion_threads %>%
    rm_emails() %>%
    rm_numbers() %>%
    rm_non_alphanumeric() %>%
    rm_whitespace %>%
    tf_lowercase()
```

# Creating the document to segment

```{r}
largest_discussion_thread <- corpus %>%
    group_by(thread) %>%
    summarise(n=n()) %>%
    arrange(desc(n)) %>%
    filter(row_number() == 1) %>%
    select(thread)

document <- curated_discussion_threads %>%
    filter(thread==largest_discussion_thread$thread)
```

# LDA Topic Model

```{r}
mallet_instances <- mallet.import(corpus$id, corpus$text, stopwords_filename)
n_topics <- 50
alpha_sum <- 5
beta <- 0.01
train_iters <- 100

topic_model <- MalletLDA(n_topics, alpha_sum, beta)
topic_model$loadDocuments(mallet_instances)
topic_model$train(train_iters)
```

# Segmenting Document

```{r}
gibbs_iters <- 2
mode <- get_topictile_mode_from_model(topic_model, gibbs_iters,
                                      "./mallet_state_file_topictile_reddit_vignette_tmp.txt")

tokens <- document %>%
    tf_tokenize()

segmented_document <- tokens %>%
    tf_topictile(stopwords=stopwords,
                 mode=mode,
                 sentence_size=20,
                 block_size=2,
                 nsegments=0,
                 liberal=TRUE) %>%
    rm_words(words=stopwords)

head(segmented_document$text, 2)
```
